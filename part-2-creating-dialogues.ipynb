{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation - Part 2\n",
    "\n",
    "Now that we have our join combinations, we need to use it with some extra information to generate our dialogues. We will also use LLMs to generate our dialogue dataset, and use human expert to check if dialogues make sense and if the sql query generated as label is also correct.\n",
    "\n",
    "Our prompt to llm will be something like this:\n",
    "```python\n",
    "\"\"\"\n",
    "Hey friend, take this joins combinations: {join_combinations}\n",
    "Consider that in this joins, we involve this tables: {tables_involved}\n",
    "\n",
    "Here are some informations about this tables: \n",
    "<big piece of context that have create table clauses and columns values examples>\n",
    "\n",
    "<Instructions>\n",
    "<Example of dialogue in the output format>\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "It works very nice because isn't hard to llms create valid SQL queries when they know the structure of the table (we give it in create table clause) and column values so llm don't hallucinate about what that can be.\n",
    "\n",
    "We have already created in last notebook the join combinations, and it is on our csv, but we need to:\n",
    "1. Extract create table clauses for each table in schema.\n",
    "2. Get from database example values available for each column.\n",
    "\n",
    "So let's do it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting table DDLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of tables found: 40\n",
      "\n",
      "Example of DDL for MONDIAL_COUNTRY:\n",
      "CREATE TABLE MONDIAL_COUNTRY (\n",
      "NAME        VARCHAR2(50),\n",
      "    CODE        VARCHAR2(4),\n",
      "    CAPITAL     VARCHAR2(50),\n",
      "    PROVINCE    VARCHAR2(50),\n",
      "    AREA        NUMBER,\n",
      "    POPULATION  NUMBER,\n",
      "    META_REPCOL VARCHAR2(4000)\n",
      "\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "with open(\"mondial/schema.txt\", \"r\") as f:\n",
    "    schema = f.read()\n",
    "\n",
    "def extract_ddls_from_schema(schema_text):\n",
    "\n",
    "    pattern = r'-- auto-generated definition\\s+create table (MONDIAL_\\w+)\\s*\\(\\s*([\\s\\S]+?)\\)\\s*/'\n",
    "    matches = re.findall(pattern, schema_text)\n",
    "    \n",
    "    ddl_dict = {}\n",
    "    for table_name, ddl_content in matches:\n",
    "        full_ddl = f\"CREATE TABLE {table_name} (\\n{ddl_content}\\n)\"\n",
    "        ddl_dict[table_name] = full_ddl\n",
    "    \n",
    "    return ddl_dict\n",
    "\n",
    "table_ddls = extract_ddls_from_schema(schema)\n",
    "\n",
    "print(f\"Total of tables found: {len(table_ddls)}\")\n",
    "print(\"\\nExample of DDL for MONDIAL_COUNTRY:\")\n",
    "print(table_ddls.get(\"MONDIAL_COUNTRY\", \"Table not found\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting column values examples\n",
    "\n",
    "### !! In my case, my company host MONDIAL on Oracle, so i'll do the stuff in next cell, you may change it to adapt to your database !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples obtained for 59 tables\n",
      "\n",
      "Example for MONDIAL_COUNTRY:\n",
      "{\n",
      "  \"NAME\": [\n",
      "    \"Austria\",\n",
      "    \"Lithuania\",\n",
      "    \"Poland\",\n",
      "    \"Faroe Islands\",\n",
      "    \"Finland\",\n",
      "    \"Ireland\",\n",
      "    \"Malta\",\n",
      "    \"Georgia\",\n",
      "    \"Bangladesh\",\n",
      "    \"Myanmar\",\n",
      "    \"Thailand\",\n",
      "    \"Israel\",\n",
      "    \"Indonesia\",\n",
      "    \"Papua New Guinea\",\n",
      "    \"Saudi Arabia\",\n",
      "    \"Syria\",\n",
      "    \"Lebanon\",\n",
      "    \"Anguilla\",\n",
      "    \"United States\",\n",
      "    \"Haiti\"\n",
      "  ],\n",
      "  \"CODE\": [\n",
      "    \"A\",\n",
      "    \"AFG\",\n",
      "    \"AG\",\n",
      "    \"AL\",\n",
      "    \"AMSA\",\n",
      "    \"AND\",\n",
      "    \"ANG\",\n",
      "    \"ARM\",\n",
      "    \"ARU\",\n",
      "    \"AUS\",\n",
      "    \"AXA\",\n",
      "    \"AZ\",\n",
      "    \"B\",\n",
      "    \"B...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import oracledb\n",
    "\n",
    "# \n",
    "with open(\"mondial/mondial_db_connection.json\", \"r\") as file:\n",
    "    db_config = json.load(file)\n",
    "\n",
    "db_host = db_config[\"DB_HOST\"]\n",
    "db_port = db_config[\"DB_PORT\"]\n",
    "db_user = db_config[\"DB_USER_NAME\"]\n",
    "db_pass = db_config[\"DB_PASS\"]\n",
    "service_name = db_config[\"SERVICE_NAME\"]\n",
    "\n",
    "dsn = f\"{db_host}:{db_port}/{service_name}\"\n",
    "connection = oracledb.connect(\n",
    "    user=db_user,\n",
    "    password=db_pass,\n",
    "    dsn=dsn\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Get all tables names\n",
    "cursor.execute(\"SELECT TABLE_NAME FROM USER_TABLES WHERE TABLE_NAME LIKE 'MONDIAL_%'\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "table_examples = {}  # Dictionary to store examples by table\n",
    "\n",
    "# For each table, search column names and examples of values\n",
    "for table in tables:\n",
    "    # Query to get column names\n",
    "    cursor.execute(f\"SELECT COLUMN_NAME FROM USER_TAB_COLUMNS WHERE TABLE_NAME = '{table}'\")\n",
    "    columns = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    columns_examples = {}\n",
    "    for column in columns:\n",
    "        # Get up to 20 distinct values\n",
    "        try:\n",
    "            cursor.execute(f\"SELECT DISTINCT {column} FROM {table} WHERE {column} IS NOT NULL FETCH FIRST 20 ROWS ONLY\")\n",
    "            examples = []\n",
    "            for row in cursor.fetchall():\n",
    "                value = row[0]\n",
    "                # If the value is a LOB, read the content\n",
    "                if isinstance(value, oracledb.LOB):\n",
    "                    value = value.read()\n",
    "                examples.append(value)\n",
    "            \n",
    "            columns_examples[column] = examples\n",
    "        except Exception as e:\n",
    "            columns_examples[column] = [\"<error to get examples>\"]\n",
    "\n",
    "    table_examples[table] = columns_examples\n",
    "\n",
    "cursor.close()\n",
    "connection.close()\n",
    "\n",
    "print(f\"Examples obtained for {len(table_examples)} tables\")\n",
    "print(\"\\nExample for MONDIAL_COUNTRY:\")\n",
    "print(json.dumps(table_examples.get(\"MONDIAL_COUNTRY\", {}), indent=2, default=str)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have the create tables, example of values, lets load the joins and get into prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of join combinations loaded: 50\n",
      "\n",
      "Example of combination:\n",
      "{\n",
      "  \"combination_id\": \"2_1\",\n",
      "  \"tables\": [\n",
      "    \"MONDIAL_GEO_MOUNTAIN\",\n",
      "    \"MONDIAL_MOUNTAIN\",\n",
      "    \"MONDIAL_MOUNTAINONISLAND\"\n",
      "  ],\n",
      "  \"combination_str\": \"MONDIAL_GEO_MOUNTAIN.['MONDIAL_GEO_MOUNTAIN.MOUNTAIN=MONDIAL_MOUNTAIN.NAME']; MONDIAL_MOUNTAINONISLAND.['MONDIAL_MOUNTAINONISLAND.MOUNTAIN=MONDIAL_MOUNTAIN.NAME']\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the file with the optimal join combinations\n",
    "join_combinations_df = pd.read_csv(\"mondial/optimal_join_combinations.csv\")\n",
    "\n",
    "# Create a list of objects for each join combination\n",
    "combination_data = []\n",
    "for idx, row in join_combinations_df.iterrows():\n",
    "    tipo = row['Type']\n",
    "    combinacao = row['Combination']\n",
    "    tabelas_str = row['Tables']\n",
    "    condicoes_join = row['Join Conditions']\n",
    "    \n",
    "    # Extract list of tables from string\n",
    "    tabelas_list = [t.strip() for t in tabelas_str.strip('\"').split(',')]\n",
    "    \n",
    "    combination_data.append({\n",
    "        \"combination_id\": f\"{tipo}_{combinacao}\",\n",
    "        \"tables\": tabelas_list,\n",
    "        \"combination_str\": condicoes_join\n",
    "    })\n",
    "\n",
    "print(f\"Total of join combinations loaded: {len(combination_data)}\")\n",
    "print(\"\\nExample of combination:\")\n",
    "print(json.dumps(combination_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we did in last notebook, for best pratices we will setup a output format to the dialogue, since it need to be well formatted to get into json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_config import LLMConfig\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "dialogue_generator = LLMConfig(provider=\"azure\").get_llm(model=\"o3-mini\", reasoning_effort=\"high\")\n",
    "\n",
    "class GroundTruth(BaseModel):\n",
    "    tables_from_schema_linking: List[str] = Field(..., description=\"List of tables involved in the query.\")\n",
    "    golden_sql: str = Field(..., description=\"Valid SQL query corresponding to the question.\")\n",
    "\n",
    "class Interaction(BaseModel):\n",
    "    interaction_id: str = Field(..., description=\"ID of the interaction.\")\n",
    "    speaker: str = Field(..., description=\"Who is speaking (always 'User').\")\n",
    "    utterance: str = Field(..., description=\"The natural language question.\")\n",
    "    intention: str = Field(..., description=\"Is the real intention of the user, it must be a natural language version of the question, considering that you don't now the global context of the dialogue or database information. Basically a compact version of the question, that agent must infer from the global context.\")\n",
    "    ground_truths: GroundTruth = Field(..., description=\"Details of the ground truth for the query.\")\n",
    "\n",
    "class Experiment(BaseModel):\n",
    "    experiment_id: str = Field(..., description=\"Unique ID of the experiment.\")\n",
    "    total_expected_interactions: int = Field(..., description=\"Total number of interactions in the dialogue.\")\n",
    "    interactions: List[Interaction] = Field(..., description=\"List of interactions in the dialogue.\")\n",
    "\n",
    "dialogue_generator_with_structured_output = dialogue_generator.with_structured_output(Experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for each join combination, we will use all information we collected to create a personalized prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de prompts criados: 50\n",
      "\n",
      "Exemplo de prompt:\n",
      "You are an advanced dialog generation agent. Your task is to generate a dialog experiment consisting of exactly 3 interactions based on the database context provided below.\n",
      "\n",
      "#Join Combination Context:\n",
      "MONDIAL_GEO_MOUNTAIN.['MONDIAL_GEO_MOUNTAIN.MOUNTAIN=MONDIAL_MOUNTAIN.NAME']; MONDIAL_MOUNTAINONISLAND.['MONDIAL_MOUNTAINONISLAND.MOUNTAIN=MONDIAL_MOUNTAIN.NAME']\n",
      "\n",
      "#Tables Involved:\n",
      "MONDIAL_GEO_MOUNTAIN, MONDIAL_MOUNTAIN, MONDIAL_MOUNTAINONISLAND\n",
      "\n",
      "#Table Details:\n",
      "\n",
      "Tabela: MONDIAL_GEO_MOUNTAIN\n",
      "DDL:\n",
      "CREATE TABLE MONDIAL_GEO_MOUNTAIN (\n",
      "MOUNTAIN    VARCHAR2(50),\n",
      "    COUNTRY     VARCHAR2(4),\n",
      "    PROVINCE    VARCHAR2(50),\n",
      "    META_REPCOL VARCHAR2(4000)\n",
      "\n",
      ")\n",
      "\n",
      "Valores de Exemplo:\n",
      "{\n",
      "  \"MOUNTAIN\": [\n",
      "    \"Chappal Waddi\",\n",
      "    \"Tamgak\",\n",
      "    \"Huascaran\",\n",
      "    \"Licancabur\",\n",
      "    \"Cerro Tristeza\",\n",
      "    \"Hiru Erregeen Mahaia\",\n",
      "    \"Moncayo\",\n",
      "    \"Jabal Sawda\",\n",
      "    \"Süphan Dagi\",\n",
      "    \"Botew\",\n",
      "    \"Kanlaon\",\n",
      "    \"Chimborazo\",\n",
      "    \"Wolf\",\n",
      "    \"Pico Mogotón\",\n",
      "    \"Pico da Bandeira\",\n",
      "    \"Kasbek\",\n",
      "    \"Hotaka-Dake\",\n",
      "    \"Saltoro Kangri\",\n",
      "    \"Pik Revoluzija\",\n",
      "    \"La Soufriere\"\n",
      "  ],\n",
      "  \"COUNTRY\": [\n",
      "    \"RI\",\n",
      "    \"CAM\",\n",
      "    \"RN\",\n",
      "    \"GR\",\n",
      "    \"KAZ\",\n",
      "    \"PE\",\n",
      "    \"IND\",\n",
      "    \"CN\",\n",
      "    \"PK\",\n",
      "    \"CH\",\n",
      "    \"CR\",\n",
      "    \"WD\",\n",
      "    \"UA\",\n",
      "    \"JOR\",\n",
      "    \"MAL\",\n",
      "    \"LAO\",\n",
      "    \"TM\",\n",
      "    \"MW\",\n",
      "    \"WAL\",\n",
      "    \"SLO\"\n",
      "  ],\n",
      "  \"PROVINCE\": [\n",
      "    \"Abuja\",\n",
      "    \"Al Qunaytirah\",\n",
      "    \"Andalucía\",\n",
      "    \"Aquitaine\",\n",
      "    \"Arequipa\",\n",
      "    \"Basilicata\",\n",
      "    \"Bío-Bío\",\n",
      "    \"Calabarzon\",\n",
      "    \"Cusco\",\n",
      "    \"Dolnoslaskie\",\n",
      "    \"Gansu\",\n",
      "    \"Georgia\",\n",
      "    \"Granma\",\n",
      "    \"Graubünden\",\n",
      "    \"Guangdong\",\n",
      "    \"Guinea\",\n",
      "    \"Haiti\",\n",
      "    \"Hokkaido\",\n",
      "    \"Ireland\",\n",
      "    \"Jalisco\"\n",
      "  ],\n",
      "  \"META_REPCOL\": [\n",
      "    \"Saltoro Kangri\",\n",
      "    \"Pik Revoluzija\",\n",
      "    \"La Soufriere\",\n",
      "    \"Santa María\",\n",
      "    \"Pic la Selle\",\n",
      "    \"Ka¿ala\",\n",
      "    \"Nevado de Colima\",\n",
      "    \"Masurai\",\n",
      "    \"Semeru\",\n",
      "    \"Changbai Shan\",\n",
      "    \"Hotaka-Dake\",\n",
      "    \"Hiru Erregeen Mahaia\",\n",
      "    \"Annapurna\",\n",
      "    \"Mt. Kosciuszko\",\n",
      "    \"Mt. Cook\",\n",
      "    \"Pico Mogotón\",\n",
      "    \"Demirkazik\",\n",
      "    \"Sarektjokko\",\n",
      "    \"Kasbek\",\n",
      "    \"Mafinga\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "Tabela: MONDIAL_MOUNTAIN\n",
      "DDL:\n",
      "CREATE TABLE MONDIAL_MOUNTAIN (\n",
      "NAME        VARCHAR2(50),\n",
      "    MOUNTAINS   VARCHAR2(50),\n",
      "    ELEVATION   NUMBER,\n",
      "    TYPE        VARCHAR2(10),\n",
      "    COORDINATES GEOCOORD,\n",
      "    META_REPCOL VARCHAR2(4000)\n",
      "\n",
      ")\n",
      "\n",
      "Valores de Exemplo:\n",
      "{\n",
      "  \"COORDINATES\": [\n",
      "    \"<error to get examples>\"\n",
      "  ],\n",
      "  \"NAME\": [\n",
      "    \"Aconcagua\",\n",
      "    \"Aenos\",\n",
      "    \"Agung\",\n",
      "    \"Alam Kuh\",\n",
      "    \"Alpamayo\",\n",
      "    \"Alto Toroni\",\n",
      "    \"Altun Shan Peak\",\n",
      "    \"Ampato\",\n",
      "    \"Anamudi\",\n",
      "    \"Andringitra\",\n",
      "    \"Annapurna\",\n",
      "    \"Aragaz\",\n",
      "    \"Ararat\",\n",
      "    \"Arma Konda\",\n",
      "    \"Asahi-Dake\",\n",
      "    \"Aso Rock\",\n",
      "    \"Asralt Khairkhan\",\n",
      "    \"Athos\",\n",
      "    \"Attavyros\",\n",
      "    \"Ausangate\"\n",
      "  ],\n",
      "  \"MOUNTAINS\": [\n",
      "    \"Changbai Shan\",\n",
      "    \"Qian Shan\",\n",
      "    \"Iran Mountains\",\n",
      "    \"Alaska Boundary Range\",\n",
      "    \"Rocky Mountains\",\n",
      "    \"Cordillera de Apaneca\",\n",
      "    \"Cordillera Occidental\",\n",
      "    \"Cordillera Negra\",\n",
      "    \"Sudety Mountains\",\n",
      "    \"Matra\",\n",
      "    \"Canary Islands\",\n",
      "    \"Cape Verdes\",\n",
      "    \"Sinai\",\n",
      "    \"Ethiopian Highlands\",\n",
      "    \"Crimean Mountains\",\n",
      "    \"Pontic Mountains\",\n",
      "    \"Asir Mountains\",\n",
      "    \"Al Hajar Mountains\",\n",
      "    \"Elburs\",\n",
      "    \"Koh-e-Baba\"\n",
      "  ],\n",
      "  \"ELEVATION\": [\n",
      "    1336,\n",
      "    3105,\n",
      "    1864,\n",
      "    2037,\n",
      "    1013,\n",
      "    2085,\n",
      "    2423,\n",
      "    2467,\n",
      "    3466,\n",
      "    813,\n",
      "    2914,\n",
      "    3142,\n",
      "    1225,\n",
      "    2779,\n",
      "    3027,\n",
      "    2334,\n",
      "    1770,\n",
      "    2228,\n",
      "    3743,\n",
      "    1917\n",
      "  ],\n",
      "  \"TYPE\": [\n",
      "    \"granite\",\n",
      "    \"volcano\",\n",
      "    \"monolith\",\n",
      "    \"volcanic\"\n",
      "  ],\n",
      "  \"META_REPCOL\": [\n",
      "    \"Changbai Shan\",\n",
      "    \"Huabo Shan\",\n",
      "    \"Kanlaon\",\n",
      "    \"Masurai\",\n",
      "    \"Semeru\",\n",
      "    \"Fuyul Sojol\",\n",
      "    \"Puncak Mandala\",\n",
      "    \"Mt. Ulawun\",\n",
      "    \"Mt. Balbi\",\n",
      "    \"Mt. Fito\",\n",
      "    \"Ka¿ala\",\n",
      "    \"Mt. Kosciuszko\",\n",
      "    \"Mt. Cook\",\n",
      "    \"Granite Peak\",\n",
      "    \"Harney Peak\",\n",
      "    \"Gannett Peak\",\n",
      "    \"Guadalupe Peak\",\n",
      "    \"Mt. Adams\",\n",
      "    \"Mt. Mazama Caldera\",\n",
      "    \"Nevado de Colima\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "Tabela: MONDIAL_MOUNTAINONISLAND\n",
      "DDL:\n",
      "CREATE TABLE MONDIAL_MOUNTAINONISLAND (\n",
      "MOUNTAIN    VARCHAR2(50),\n",
      "    ISLAND      VARCHAR2(50),\n",
      "    META_REPCOL VARCHAR2(4000)\n",
      "\n",
      ")\n",
      "\n",
      "Valores de Exemplo:\n",
      "{\n",
      "  \"MOUNTAIN\": [\n",
      "    \"Chikurachki\",\n",
      "    \"Fuyul Sojol\",\n",
      "    \"Hotaka-Dake\",\n",
      "    \"Jabal Hajhir\",\n",
      "    \"Kanlaon\",\n",
      "    \"Ka¿ala\",\n",
      "    \"Kita-Dake\",\n",
      "    \"Krenizyn\",\n",
      "    \"La Soufriere\",\n",
      "    \"Liamuiga\",\n",
      "    \"Masurai\",\n",
      "    \"Mt. Balbi\",\n",
      "    \"Mt. Cook\",\n",
      "    \"Mt. Fito\",\n",
      "    \"Mt. Ulawun\",\n",
      "    \"Pic la Selle\",\n",
      "    \"Pico Ruivo\",\n",
      "    \"Piton de la Fournaise\",\n",
      "    \"Puncak Mandala\",\n",
      "    \"Queen Marys Peak\"\n",
      "  ],\n",
      "  \"ISLAND\": [\n",
      "    \"Hokkaido\",\n",
      "    \"Rhodos\",\n",
      "    \"Ireland\",\n",
      "    \"Isla da Ometepe\",\n",
      "    \"Lefkas\",\n",
      "    \"Maui\",\n",
      "    \"Grand Comoro\",\n",
      "    \"Krenizyn\",\n",
      "    \"Unalaska\",\n",
      "    \"Upolu\",\n",
      "    \"Vanua Levu\",\n",
      "    \"Goodenough Island\",\n",
      "    \"Hispaniola\",\n",
      "    \"Chios\",\n",
      "    \"Mallorca\",\n",
      "    \"Sardegna\",\n",
      "    \"Ambon\",\n",
      "    \"Unimak\",\n",
      "    \"Montserrat\",\n",
      "    \"Madagaskar\"\n",
      "  ],\n",
      "  \"META_REPCOL\": [\n",
      "    \"Chikurachki\",\n",
      "    \"Fuyul Sojol\",\n",
      "    \"Hotaka-Dake\",\n",
      "    \"Jabal Hajhir\",\n",
      "    \"Kanlaon\",\n",
      "    \"Ka¿ala\",\n",
      "    \"Kita-Dake\",\n",
      "    \"Krenizyn\",\n",
      "    \"La Soufriere\",\n",
      "    \"Liamuiga\",\n",
      "    \"Masurai\",\n",
      "    \"Mt. Balbi\",\n",
      "    \"Mt. Cook\",\n",
      "    \"Mt. Fito\",\n",
      "    \"Mt. Ulawun\",\n",
      "    \"Pic la Selle\",\n",
      "    \"Pico Ruivo\",\n",
      "    \"Piton de la Fournaise\",\n",
      "    \"Puncak Mandala\",\n",
      "    \"Queen Marys Peak\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "Generate an experiment following the JSON structure below. The experiment must include exactly 3 user interactions, each with:\n",
      "- \"interaction_id\": a string identifier.\n",
      "- \"speaker\": always \"User\".\n",
      "- \"utterance\": the user's question expressed naturally.\n",
      "- \"intention\": an exact or contextually refined rewrite of the \"utterance\" that reflects what the user really intends. If no additional context is needed, \"intention\" should be identical to \"utterance\". For follow-up queries, integrate the previous context to form a refined natural language version.\n",
      "- \"ground_truths\": an object containing:\n",
      "  - \"tables_from_schema_linking\": a list of the tables involved.\n",
      "  - \"danke_sql\": a valid SQL query that can be executed on the provided schema. The SQL must be accurate and specific, and if sample values are used, they should serve only as filters—not to overly restrict or specify the general question.\n",
      "\n",
      "Expected JSON format, THAT'S AN EXAMPLE, YOU MUST FOLLOW THIS STRUCTURE, NOTHING ELSE:\n",
      "{\n",
      "  \"experiment_id\": \"1\",\n",
      "  \"total_expected_interactions\": 3,\n",
      "  \"interactions\": [\n",
      "    {\n",
      "      \"interaction_id\": \"1\",\n",
      "      \"speaker\": \"User\",\n",
      "      \"utterance\": \"What are the biggest city in the world?\",\n",
      "      \"intention\": \"What are the biggest city in the world?\",\n",
      "      \"ground_truths\": {\n",
      "         \"tables_from_schema_linking\": [ ... ],\n",
      "         \"danke_sql\": \"<Valid SQL query corresponding to the question>\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"interaction_id\": \"2\",\n",
      "      \"speaker\": \"User\",\n",
      "      \"utterance\": \"What are the area of this city?\",\n",
      "      \"intention\": \"What are the area of <city retrieved in the previous interaction>?\",\n",
      "      \"ground_truths\": {\n",
      "         \"tables_from_schema_linking\": [ ... ],\n",
      "         \"danke_sql\": \"<Valid SQL query for the follow-up question>\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"interaction_id\": \"3\",\n",
      "      \"speaker\": \"User\",\n",
      "      \"utterance\": \"And tell me also in wich continent it is?\",\n",
      "      \"intention\": \"In wich continent is <city retrieved in the previous interaction>?\",\n",
      "      \"ground_truths\": {\n",
      "         \"tables_from_schema_linking\": [ ... ],\n",
      "         \"danke_sql\": \"<Valid SQL query for the additional question>\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "As you see, follow up questions are always related to the previous one, and the intention is a natural language version of the question, considering that you don't now the global context of the dialogue.\n",
      "\n",
      "You must pay extra attention to the \"intention\" field, it must be a natural language version of the question, considering that you don't now the global context of the dialogue or database information, so don't use database information in the intention.\n",
      "\n",
      "Try to generate a dialogue that the answer is not a giant table scan, more specific the better....\n"
     ]
    }
   ],
   "source": [
    "# Prompts for each join combination\n",
    "prompts = []\n",
    "for idx, comb_obj in enumerate(combination_data):\n",
    "    experiment_id = str(idx + 1)  # IDs start from 1\n",
    "    join_str = comb_obj[\"combination_str\"]\n",
    "    tables_involved = comb_obj[\"tables\"]\n",
    "    joins_len = len(set(tables_involved))\n",
    "\n",
    "    # For each involved table, we search the DDL and examples\n",
    "    table_contexts = \"\"\n",
    "    for table in tables_involved:\n",
    "        ddl_text = table_ddls.get(table, \"DDL not found.\")\n",
    "        examples = table_examples.get(table, {})\n",
    "        \n",
    "        # Convert datetime objects to string before serializing to JSON\n",
    "        processed_examples = {}\n",
    "        for key, value in examples.items():\n",
    "            if isinstance(value, list):\n",
    "                processed_examples[key] = [str(item) if hasattr(item, 'isoformat') else item for item in value]\n",
    "            else:\n",
    "                processed_examples[key] = str(value) if hasattr(value, 'isoformat') else value\n",
    "        \n",
    "        examples_str = json.dumps(processed_examples, indent=2, ensure_ascii=False)\n",
    "        table_contexts += f\"\\nTabela: {table}\\nDDL:\\n{ddl_text}\\n\\nValores de Exemplo:\\n{examples_str}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an advanced dialog generation agent. Your task is to generate a dialog experiment consisting of exactly {joins_len} interactions based on the database context provided below.\n",
    "\n",
    "#Join Combination Context:\n",
    "{join_str}\n",
    "\n",
    "#Tables Involved:\n",
    "{', '.join(tables_involved)}\n",
    "\n",
    "#Table Details:\n",
    "{table_contexts}\n",
    "\n",
    "Generate an experiment following the JSON structure below. The experiment must include exactly 3 user interactions, each with:\n",
    "- \"interaction_id\": a string identifier.\n",
    "- \"speaker\": always \"User\".\n",
    "- \"utterance\": the user's question expressed naturally.\n",
    "- \"intention\": an exact or contextually refined rewrite of the \"utterance\" that reflects what the user really intends. If no additional context is needed, \"intention\" should be identical to \"utterance\". For follow-up queries, integrate the previous context to form a refined natural language version.\n",
    "- \"ground_truths\": an object containing:\n",
    "  - \"tables_from_schema_linking\": a list of the tables involved.\n",
    "  - \"danke_sql\": a valid SQL query that can be executed on the provided schema. The SQL must be accurate and specific, and if sample values are used, they should serve only as filters—not to overly restrict or specify the general question.\n",
    "\n",
    "Expected JSON format, THAT'S AN EXAMPLE, YOU MUST FOLLOW THIS STRUCTURE, NOTHING ELSE:\n",
    "{{\n",
    "  \"experiment_id\": \"{experiment_id}\",\n",
    "  \"total_expected_interactions\": 3,\n",
    "  \"interactions\": [\n",
    "    {{\n",
    "      \"interaction_id\": \"1\",\n",
    "      \"speaker\": \"User\",\n",
    "      \"utterance\": \"What are the biggest city in the world?\",\n",
    "      \"intention\": \"What are the biggest city in the world?\",\n",
    "      \"ground_truths\": {{\n",
    "         \"tables_from_schema_linking\": [ ... ],\n",
    "         \"danke_sql\": \"<Valid SQL query corresponding to the question>\"\n",
    "      }}\n",
    "    }},\n",
    "    {{\n",
    "      \"interaction_id\": \"2\",\n",
    "      \"speaker\": \"User\",\n",
    "      \"utterance\": \"What are the area of this city?\",\n",
    "      \"intention\": \"What are the area of <city retrieved in the previous interaction>?\",\n",
    "      \"ground_truths\": {{\n",
    "         \"tables_from_schema_linking\": [ ... ],\n",
    "         \"danke_sql\": \"<Valid SQL query for the follow-up question>\"\n",
    "      }}\n",
    "    }},\n",
    "    {{\n",
    "      \"interaction_id\": \"3\",\n",
    "      \"speaker\": \"User\",\n",
    "      \"utterance\": \"And tell me also in wich continent it is?\",\n",
    "      \"intention\": \"In wich continent is <city retrieved in the previous interaction>?\",\n",
    "      \"ground_truths\": {{\n",
    "         \"tables_from_schema_linking\": [ ... ],\n",
    "         \"danke_sql\": \"<Valid SQL query for the additional question>\"\n",
    "      }}\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "As you see, follow up questions are always related to the previous one, and the intention is a natural language version of the question, considering that you don't now the global context of the dialogue.\n",
    "\n",
    "You must pay extra attention to the \"intention\" field, it must be a natural language version of the question, considering that you don't now the global context of the dialogue or database information, so don't use database information in the intention.\n",
    "\n",
    "Try to generate a dialogue that the answer is not a giant table scan, more specific the better.\n",
    "\"\"\"\n",
    "\n",
    "    prompts.append((experiment_id, prompt.strip()))\n",
    "\n",
    "print(f\"Total de prompts criados: {len(prompts)}\")\n",
    "print(\"\\nExemplo de prompt:\")\n",
    "print(prompts[0][1] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I really recommend you to read the example prompt that I've printed, to understand all the information we are prompting to LLM. Now for each prompt created, let's send to llm and generate the dialogues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing experiment 1/50 (ID: 1)\n",
      "Experiment generated: 0 (ID: 1)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 2/50 (ID: 2)\n",
      "Experiment generated: 1 (ID: 2)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 3/50 (ID: 3)\n",
      "Experiment generated: 2 (ID: 3)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 4/50 (ID: 4)\n",
      "Experiment generated: 3 (ID: 4)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 5/50 (ID: 5)\n",
      "Experiment generated: 4 (ID: 5)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 6/50 (ID: 6)\n",
      "Experiment generated: 5 (ID: 6)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 7/50 (ID: 7)\n",
      "Experiment generated: 6 (ID: 7)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 8/50 (ID: 8)\n",
      "Experiment generated: 7 (ID: 8)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 9/50 (ID: 9)\n",
      "Experiment generated: 8 (ID: 9)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 10/50 (ID: 10)\n",
      "Experiment generated: 9 (ID: 10)\n",
      "Dataset updated and saved in mondial/dialogue_dataset.json\n",
      "Processing experiment 11/50 (ID: 11)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_file = \"mondial/dialogue_dataset.json\"\n",
    "\n",
    "# Check if the file already exists and load existing data\n",
    "dataset = []\n",
    "last_processed_index = -1\n",
    "if os.path.exists(output_file):\n",
    "    try:\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            existing_data = json.load(f)\n",
    "            dataset = existing_data.get(\"dataset\", [])\n",
    "            \n",
    "            # Find the last processed experiment_id (good for continue the process in case of interruption)\n",
    "            if dataset:\n",
    "                processed_ids = [exp.get(\"experiment_id\") for exp in dataset if \"experiment_id\" in exp]\n",
    "                for i, (exp_id, _) in enumerate(prompts):\n",
    "                    if exp_id in processed_ids:\n",
    "                        last_processed_index = i\n",
    "                \n",
    "                print(f\"Found {len(dataset)} already processed experiments. Continuing from index {last_processed_index + 1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error to load existing file: {e}\")\n",
    "        print(\"Starting with a new dataset\")\n",
    "\n",
    "# Process the remaining prompts\n",
    "for i, (experiment_id, prompt) in enumerate(prompts):\n",
    "    # Skip already processed\n",
    "    if i <= last_processed_index:\n",
    "        continue\n",
    "    \n",
    "    print(f\"Processing experiment {i+1}/{len(prompts)} (ID: {experiment_id})\")\n",
    "    \n",
    "    try:\n",
    "        # Call to LLM with structured output based on Experiment model\n",
    "        experiment_output = dialogue_generator_with_structured_output.invoke(prompt)\n",
    "        experiment_dict = experiment_output.dict()\n",
    "\n",
    "        # If necessary, adjust the key name for the tables\n",
    "        for interaction in experiment_dict.get(\"interactions\", []):\n",
    "            if \"ground_truths\" in interaction:\n",
    "                gt = interaction[\"ground_truths\"]\n",
    "                if \"tables_envolved\" in gt:\n",
    "                    gt[\"tables_from_schema_linking\"] = gt.pop(\"tables_envolved\")\n",
    "\n",
    "        dataset.append(experiment_dict)\n",
    "        print(f\"Experiment generated: {i} (ID: {experiment_id})\")\n",
    "        \n",
    "        # Salvar após cada experimento para evitar perda de dados\n",
    "        final_dataset = {\"dataset\": dataset}\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(final_dataset, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Dataset updated and saved in {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error to process experiment {experiment_id}: {e}\")\n",
    "\n",
    "print(f\"Completed. Total of {len(dataset)} experiments in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
